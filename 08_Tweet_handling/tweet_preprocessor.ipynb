{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5SzZYXq3ER6b"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "id": "cOqUHRECEX9o",
    "outputId": "4ec4770f-ea54-4ee0-e3a4-2661629c52ee"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install ekphrasis\n",
    "from ekphrasis.classes.segmenter import Segmenter\n",
    "\n",
    "# to leverage word statistics from Twitter\n",
    "seg_tw = Segmenter(corpus = \"twitter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "cvfnKs-dEZwN",
    "outputId": "06a34c8d-e6e1-4842-8f9f-dca10138b77c"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install tweet-preprocessor\n",
    "import preprocessor as tweet_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Wk2BUZTuEdN1",
    "outputId": "86fc9ba1-de39-4992-a083-9a760455c35a"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Reference: https://github.com/NeelShah18/emot\n",
    "\n",
    "!pip install emot --upgrade\n",
    "import emot\n",
    "\n",
    "emot_obj = emot.core.emot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fD2Ksnv6EmaS"
   },
   "outputs": [],
   "source": [
    "def make_list(proc_obj):\n",
    "    if proc_obj == None:\n",
    "        return []\n",
    "\n",
    "    store = []\n",
    "    for unit in proc_obj:\n",
    "        store.append(unit.match)\n",
    "    \n",
    "    return store\n",
    "\n",
    "def emotext(text):\n",
    "    for emot in UNICODE_EMO:\n",
    "        text = text.replace(emot, \"_\".join(UNICODE_EMO[emot].replace(\",\", \"\").replace(\":\", \"\").split()))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nNNoGyv8E5th"
   },
   "outputs": [],
   "source": [
    "# Initializing Lists\n",
    "\n",
    "datapoints_count = 0\n",
    "\n",
    "tweets = []\n",
    "cleaned_tweet_texts = []\n",
    "tokenized_tweets = []\n",
    "\n",
    "hashtags = []\n",
    "smileys = []\n",
    "emojis = []\n",
    "\n",
    "urls = []\n",
    "\n",
    "mentions = []\n",
    "\n",
    "numbers = []\n",
    "\n",
    "reserveds = []\n",
    "\n",
    "task_1_labels = []\n",
    "task_2_labels = []\n",
    "\n",
    "tweet_ids = []\n",
    "hasoc_ID = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nsKZHtQ-FvOw"
   },
   "outputs": [],
   "source": [
    "def strip_list(listie):\n",
    "    stripped = []\n",
    "    \n",
    "    for item in listie:\n",
    "        stripped.append(item.strip())\n",
    "    return stripped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0TuJvskdE95H"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Datapoints: 3708\n"
     ]
    }
   ],
   "source": [
    "file_name = \"./english.csv\"\n",
    "\n",
    "file = open(file_name, 'r+', encoding = 'utf-8')\n",
    "\n",
    "file_reader = csv.reader(file, delimiter = ',')\n",
    "\n",
    "see_index = True\n",
    "\n",
    "for line in file_reader:\n",
    "    assert len(line) == 5\n",
    "    \n",
    "    if see_index == True:\n",
    "        see_index = False\n",
    "        continue\n",
    "\n",
    "    datapoints_count += 1\n",
    "    \n",
    "    tweet_ids.append(line[0])\n",
    "    task_1_labels.append(line[2])\n",
    "    task_2_labels.append(line[3])\n",
    "    hasoc_ID.append(line[4])\n",
    "    tweets.append(line[1].replace('\\n', ' '))\n",
    "\n",
    "    parse_obj = tweet_proc.parse(line[1].replace('\\n', ' '))\n",
    "    \n",
    "    tokenized_tweets.append(tweet_proc.tokenize(line[1].replace('\\n', ' ')))\n",
    "    \n",
    "    cleaned_tweet_texts.append(tweet_proc.clean(line[1].replace('\\n', ' ')))\n",
    "    \n",
    "    hashtags.append(strip_list(make_list(parse_obj.hashtags)))\n",
    "    smileys.append(strip_list(make_list(parse_obj.smileys)))\n",
    "    emojis.append(strip_list(make_list(parse_obj.emojis)))\n",
    "    \n",
    "    urls.append(strip_list(make_list(parse_obj.urls)))\n",
    "    \n",
    "    mentions.append(strip_list(make_list(parse_obj.mentions)))\n",
    "    \n",
    "    numbers.append(strip_list(make_list(parse_obj.numbers)))\n",
    "    \n",
    "    reserveds.append(strip_list(make_list(parse_obj.reserved)))\n",
    "\n",
    "print(\"Number of Datapoints: \" + str(datapoints_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "id": "Gd_fy5REFxUX",
    "outputId": "f70d9fb9-2246-4c8d-f59e-5574161390b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets:\n",
      "['RT @laurabranigan: All READY for @StLouisBlues Game 5!!🙂 GO BLUES!!!🙂💙🎶💛🏒 ~ Kathy Golik, Other Half Entertainment #LGB #PlayGloria #LauraBr…', 'RT @shannon49170750: Me staring at my carmex only after applying it 3 minutes ago   me: Don’t do it  Don’t do it  Don’t do it  Don’t do it…', 'someone said I look like max theiriot and now no one can tell me shit about my looks. best compliment ever.']\n",
      "\n",
      "Cleaned Texts:\n",
      "[': All READY for Game !! GO BLUES!!! ~ Kathy Golik, Other Half Entertainment', ': Me staring at my carmex only after applying it minutes ago me: Dont do it Dont do it Dont do it Dont do it', 'someone said I look like max theiriot and now no one can tell me shit about my looks. best compliment ever.']\n",
      "\n",
      "Hashtags:\n",
      "[['#LGB', '#PlayGloria', '#LauraBr'], [], []]\n",
      "\n",
      "Smileys:\n",
      "[[], [], []]\n",
      "\n",
      "Emojis:\n",
      "[['🙂', '🙂', '💙', '🎶', '💛', '🏒'], [], []]\n",
      "\n",
      "Urls:\n",
      "[[], [], []]\n",
      "\n",
      "Mentions:\n",
      "[['@laurabranigan', '@StLouisBlues'], ['@shannon49170750'], []]\n",
      "\n",
      "Numbers:\n",
      "[['5'], ['3'], []]\n",
      "\n",
      "Reserved Words:\n",
      "[['RT'], ['RT'], []]\n",
      "\n",
      "Task Labels:\n",
      "['NOT', 'NOT', 'HOF']\n",
      "['NONE', 'NONE', 'PRFN']\n"
     ]
    }
   ],
   "source": [
    "# Viewing Created Dataset\n",
    "display_size = 3\n",
    "start = 100\n",
    "\n",
    "print(\"Tweets:\")\n",
    "print(tweets[start: start + display_size], end = '\\n\\n')\n",
    "\n",
    "print(\"Cleaned Texts:\")\n",
    "print(cleaned_tweet_texts[start: start + display_size], end = '\\n\\n')\n",
    "\n",
    "print(\"Hashtags:\")\n",
    "print(hashtags[start: start + display_size], end = '\\n\\n')\n",
    "\n",
    "print(\"Smileys:\")\n",
    "print(smileys[start: start + display_size], end = '\\n\\n')\n",
    "\n",
    "print(\"Emojis:\")\n",
    "print(emojis[start: start + display_size], end = '\\n\\n')\n",
    "\n",
    "print(\"Urls:\")\n",
    "print(urls[start: start + display_size], end = '\\n\\n')\n",
    "\n",
    "print(\"Mentions:\")\n",
    "print(mentions[start: start + display_size], end = '\\n\\n')\n",
    "\n",
    "print(\"Numbers:\")\n",
    "print(numbers[start: start + display_size], end = '\\n\\n')\n",
    "\n",
    "print(\"Reserved Words:\")\n",
    "print(reserveds[start: start + display_size], end = '\\n\\n')\n",
    "\n",
    "print(\"Task Labels:\")\n",
    "print(task_1_labels[start: start + display_size])\n",
    "print(task_2_labels[start: start + display_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u6sOeB9UWE7p"
   },
   "source": [
    "### Example\n",
    "\n",
    "#### Tweet Text\n",
    "'RT @jeonggukpics: Don’t disturb please, he is enjoying his snacks while making those little dance 😭😂😂😭💜  #BBMAsTopSocial BTS #JUNGKOOK #정국…'\n",
    "\n",
    "#### Clean Text\n",
    "': Dont disturb please, he is enjoying his snacks while making those little dance BTS'\n",
    "\n",
    "#### Emojis\n",
    "'['😭', '😂', '😂', '😭', '💜']'\n",
    "\n",
    "#### Hashtags\n",
    "''#BBMAsTopSocial', '#JUNGKOOK', '#정국''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "LUrX_FPcIVQR",
    "outputId": "74d61f00-c87e-48d7-98a2-2ee5d8dc3633"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_meaning(\"😂\"): face_with_tears_of_joy\n"
     ]
    }
   ],
   "source": [
    "# Generating Emoji Texts\n",
    "\n",
    "def get_meaning(item):\n",
    "    \n",
    "    fields = emot_obj.emoji(item)\n",
    "    \n",
    "    if len(fields['mean']) == 0:\n",
    "        return ''\n",
    "    \n",
    "    return fields['mean'][0].strip(':')\n",
    "\n",
    "# Testing\n",
    "print('get_meaning(\"😂\"): ' + str(get_meaning(\"😂\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoji Descriptions:\n",
      "[['face with tears of joy', 'face with tears of joy'], [], [], ['face with tears of joy'], []]\n"
     ]
    }
   ],
   "source": [
    "emoji_texts = []\n",
    "\n",
    "for emo_list in emojis:\n",
    "    texts = []\n",
    "    \n",
    "    for emoji in emo_list:\n",
    "        text = get_meaning(emoji)\n",
    "        texts.append(text.replace('_', ' '))\n",
    "    \n",
    "    emoji_texts.append(texts)\n",
    "    \n",
    "print(\"Emoji Descriptions:\")\n",
    "print(emoji_texts[0: 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "9PWvCf65IXQT",
    "outputId": "93f0c3df-ccb5-415e-c26b-a743d2157c80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmented Hashtags: \n",
      "[['lgb', 'play gloria', 'laura br'], [], []]\n"
     ]
    }
   ],
   "source": [
    "# Segmenting Hashtags\n",
    "\n",
    "segmented_hashtags = []\n",
    "\n",
    "for hashset in hashtags:\n",
    "    segmented_set = []\n",
    "    for tag in hashset:\n",
    "        word = tag[1: ]\n",
    "        # removing the hash symbol\n",
    "        segmented_set.append(seg_tw.segment(word))\n",
    "    segmented_hashtags.append(segmented_set)\n",
    "\n",
    "print(\"Segmented Hashtags: \")\n",
    "print(segmented_hashtags[start: start + display_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "UGPepLXoJsqR"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "name = 'en.json'\n",
    "data_dict = {}\n",
    "\n",
    "data_dict['tweet_id'] = tweet_ids\n",
    "data_dict['task_1'] = task_1_labels\n",
    "data_dict['task_2'] = task_2_labels\n",
    "data_dict['hasoc_id'] = hasoc_ID\n",
    "data_dict['full_tweet'] = tweets\n",
    "data_dict['tweet_cleaned_text'] = cleaned_tweet_texts\n",
    "data_dict['hashtags'] = hashtags\n",
    "data_dict['smiley'] = smileys\n",
    "data_dict['emoji'] = emojis\n",
    "data_dict['url'] = urls\n",
    "data_dict['mentions'] = mentions\n",
    "data_dict['numerals'] = numbers\n",
    "data_dict['reserved_word'] = reserveds\n",
    "data_dict['emotext'] = emoji_texts\n",
    "data_dict['segmented_hash'] = segmented_hashtags\n",
    "\n",
    "with open(name, 'w+') as f:\n",
    "    json.dump(data_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "3FUMYDP9IaXe"
   },
   "outputs": [],
   "source": [
    "# That's it"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tweet_processor.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
